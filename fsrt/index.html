<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
<!--  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>-->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


<!--  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">-->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
<!--  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">-->
  <!-- Keywords for your paper to be indexed by-->
<!--  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">-->


  <title>FSRT: Facial Scene Representation Transformer</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  
  <style>
@media (max-width: 840px) {
  .dynbr {
    display: none;
  }
}
  </style>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-3 publication-title">
              FSRT: Facial Scene Representation Transformer<br class="dynbr">
              for Face Reenactment from Factorized Appearance,<br class="dynbr">
              Head-pose, and Facial Expression Features
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=Ci1KBJwAAAAJ&hl=en" target="_blank">Andre Rochow</a><sup></sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.de/citations?user=3nGA1pMAAAAJ&hl=en" target="_blank">Max Schwarz</a><sup></sup>,</span>
                  <span class="author-block">
                    <a href="https://www.ais.uni-bonn.de/behnke/" target="_blank">Sven Behnke</a>
                  </span>
                  </div>

                  <div class="is-size-6 publication-authors">
                    <span class="author-block">Autonomous Intelligent Systems - Computer Science Institute VI and Center for Robotics, University of Bonn, Germany<br>Lamarr Institute for Machine Learning and Artificial Intelligence, Germany</span>
                  </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">CVPR 2024</span>
                  </div>
                  <div class="column has-text-centered">
                    <div class="publication-links">

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/andrerochow/fsrt" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2404.09736" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/supplementary_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The task of face reenactment is to transfer the head motion and facial expressions from a driving video to the appearance of a source image, which may be of a different person (cross-reenactment). Most existing methods are CNN-based and estimate optical flow from the source image to the current driving frame, which is then inpainted and refined to produce the output animation. We propose a transformer-based encoder for computing a set-latent representation of the source image(s). We then predict the output color of a query pixel using a transformer-based decoder, which is conditioned with keypoints and a facial expression vector extracted from the driving frame. Latent representations of the source person are learned in a self-supervised manner that factorize their appearance, head pose, and facial expressions. Thus, they are perfectly suited for cross-reenactment. In contrast to most related work, our method naturally extends to multiple source images and can thus adapt to person-specific facial dynamics. We also propose data augmentation and regularization schemes that are necessary to prevent overfitting and support generalizability of the learned representations. We evaluated our approach in a randomized user study. The results indicate superior performance compared to the state-of-the-art in terms of motion transfer quality and temporal consistency.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Paper abstract -->


<!-- Method Overview -->
<section class="hero is-small">
  <div class="container is-max-desktop">
    <div class="hero-body">
    <h2 class="title is-3">Method Overview</h2>
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/method_overview.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-justified">
        Overview of our method with relative motion transfer. The source image(s) are encoded along with keypoints, capturing head pose, and facial expression vectors to a set-latent representation of the source person. The decoder attends this representation for a query pixel, conditioned on keypoints and a facial expression vector extracted from the driving frame. Images and videos from the VoxCeleb test set.
      </h2>
    </div>
  </div>
</section>
<!-- End Method Overview -->


<!-- Generalizing to CelebA-HQ -->
<section class="hero is-small">
  <div class="container is-max-desktop">
    <div class="hero-body">
    <h2 class="title is-3">Generalizing to CelebA-HQ</h2>
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/CelebA-HQ_rel_1.mp4"
        type="video/mp4">
      </video>
      <br>
      <br>
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/CelebA-HQ_rel_2.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-justified">
        Our model generalizes to source images from the CelebA-HQ dataset (top rows) and driving videos from the official VoxCeleb2 test set (left).
      </h2>
    </div>
  </div>
</section>
<!-- End Generalizing to CelebA-HQ -->


<!-- Architecture -->
<section class="hero is-small">
  <div class="container is-max-desktop">
    <div class="hero-body">
    <h2 class="title is-3">Architecture</h2>
      <img src="static/images/architecture.svg" width="100%"/>
      <h2 class="subtitle has-text-justified">
      Given the driving frame and source images, we extract facial keypoints and latent expression vectors. Extracted source information are used to generate the input representation of the Patch CNN. The encoder infers the set-latent source face  representation from the patch embeddings. The decoder is applied for each query pixel individually and is conditioned with the driving keypoints and the latent driving expression vector.
      </h2>
    </div>
  </div>
</section>
<!-- End Architecture -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{rochow2024fsrt,
  title={{FSRT}: Facial Scene Representation Transformer for Face Reenactment from Factorized Appearance, Head-pose, and Facial Expression Features},
  author={Rochow, Andre and Schwarz, Max and Behnke, Sven},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={7716--7726},
  year={2024}
}
</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
